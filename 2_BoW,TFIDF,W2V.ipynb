{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Bag of words-1\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "#Take the input\n",
        "text1 = [\"\"\"I want to go to school . I am going to school now .\"\"\"]\n",
        "\n",
        "tokens1 = [[item for item in line.split()] for line in text1]\n",
        "g_dict1 = corpora.Dictionary(tokens1)\n",
        "\n",
        "#Count number of tokens\n",
        "print(\"The dictionary has: \" +str(len(g_dict1)) + \" tokens\\n\")\n",
        "print(g_dict1.token2id)\n",
        "\n",
        "#Bag of words\n",
        "g_bow =[g_dict1.doc2bow(token, allow_update = True) for token in tokens1]\n",
        "print(\"Bag of Words : \", g_bow)\n",
        "\n",
        "\n",
        "#TFIDF-2\n",
        "import pprint\n",
        "import numpy as np\n",
        "from gensim import corpora,models\n",
        "from gensim.utils import simple_preprocess\n",
        "text = [\"The food is excellent but the service can be better\",\n",
        "        \"The food is always delicious and loved the service\",\n",
        "        \"The food was mediocre and the service was terrible\"]\n",
        "\n",
        "g_dict = corpora.Dictionary([simple_preprocess(line) for line in text])\n",
        "g_bow = [g_dict.doc2bow(simple_preprocess(line)) for line in text]\n",
        "\n",
        "print(\"Dictionary : \")\n",
        "for item in g_bow:\n",
        "    print([[g_dict[id], freq] for id, freq in item])\n",
        "\n",
        "g_tfidf = models.TfidfModel(g_bow, smartirs='ntc')\n",
        "print(\" \")\n",
        "print(\"TF-IDF Vector:\")\n",
        "for item in g_tfidf[g_bow]:\n",
        "    print([[g_dict[id], np.around(freq, decimals=2)] for id, freq in item])\n",
        "\n",
        "\n",
        "\n",
        "#Word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "data = [\n",
        "    \"Open Visual Studio Code.\",\n",
        "    \"Import the NLTK library\",\n",
        "    \"Apart from individual data packages\",\n",
        "    \"word2vec is a popular word\",\n",
        "]\n",
        "\n",
        "# Tokenize the text data (split sentences into words)\n",
        "tokenized_data = [sentence.split() for sentence in data]\n",
        "\n",
        "# Create a Word2Vec model\n",
        "w2v_model = Word2Vec(tokenized_data, min_count=0, workers=cpu_count())\n",
        "\n",
        "# Find the most similar words to 'word'\n",
        "similar_words = w2v_model.wv.most_similar('word')\n",
        "\n",
        "for word, score in similar_words:\n",
        "    print(f\"{word}: {score}\")\n",
        ""
      ],
      "metadata": {
        "id": "b6g8lbH54uLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVWD_Day3C_F",
        "outputId": "92588840-f56a-4cce-a0ad-0cb162ba1f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dictionary has: 9 tokens\n",
            "\n",
            "{'.': 0, 'I': 1, 'am': 2, 'go': 3, 'going': 4, 'now': 5, 'school': 6, 'to': 7, 'want': 8}\n",
            "Bag of Words :  [[(0, 2), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 3), (8, 1)]]\n"
          ]
        }
      ],
      "source": [
        "#Bag of words-1\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "#Take the input\n",
        "text1 = [\"\"\"I want to go to school . I am going to school now .\"\"\"]\n",
        "\n",
        "tokens1 = [[item for item in line.split()] for line in text1]\n",
        "g_dict1 = corpora.Dictionary(tokens1)\n",
        "\n",
        "#Count number of tokens\n",
        "print(\"The dictionary has: \" +str(len(g_dict1)) + \" tokens\\n\")\n",
        "print(g_dict1.token2id)\n",
        "\n",
        "#Bag of words\n",
        "g_bow =[g_dict1.doc2bow(token, allow_update = True) for token in tokens1]\n",
        "print(\"Bag of Words : \", g_bow)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TFIDF-2\n",
        "import pprint\n",
        "import numpy as np\n",
        "from gensim import corpora,models\n",
        "from gensim.utils import simple_preprocess\n",
        "text = [\"The food is excellent but the service can be better\",\n",
        "        \"The food is always delicious and loved the service\",\n",
        "        \"The food was mediocre and the service was terrible\"]\n",
        "\n",
        "g_dict = corpora.Dictionary([simple_preprocess(line) for line in text])\n",
        "g_bow = [g_dict.doc2bow(simple_preprocess(line)) for line in text]\n",
        "\n",
        "print(\"Dictionary : \")\n",
        "for item in g_bow:\n",
        "    print([[g_dict[id], freq] for id, freq in item])\n",
        "\n",
        "g_tfidf = models.TfidfModel(g_bow, smartirs='ntc')\n",
        "print(\" \")\n",
        "print(\"TF-IDF Vector:\")\n",
        "for item in g_tfidf[g_bow]:\n",
        "    print([[g_dict[id], np.around(freq, decimals=2)] for id, freq in item])\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7pSQflH4uCF",
        "outputId": "877a2781-1372-45d2-fd24-60e0693b5ca2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary : \n",
            "[['be', 1], ['better', 1], ['but', 1], ['can', 1], ['excellent', 1], ['food', 1], ['is', 1], ['service', 1], ['the', 2]]\n",
            "[['food', 1], ['is', 1], ['service', 1], ['the', 2], ['always', 1], ['and', 1], ['delicious', 1], ['loved', 1]]\n",
            "[['food', 1], ['service', 1], ['the', 2], ['and', 1], ['mediocre', 1], ['terrible', 1], ['was', 2]]\n",
            " \n",
            "TF-IDF Vector:\n",
            "[['be', 0.43], ['better', 0.43], ['but', 0.43], ['can', 0.43], ['excellent', 0.43], ['food', 0.09], ['is', 0.21], ['service', 0.09], ['the', 0.18]]\n",
            "[['food', 0.11], ['is', 0.26], ['service', 0.11], ['the', 0.21], ['always', 0.52], ['and', 0.26], ['delicious', 0.52], ['loved', 0.52]]\n",
            "[['food', 0.08], ['service', 0.08], ['the', 0.16], ['and', 0.2], ['mediocre', 0.39], ['terrible', 0.39], ['was', 0.78]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "data = [\n",
        "    \"Open Visual Studio Code.\",\n",
        "    \"Import the NLTK library\",\n",
        "    \"Apart from individual data packages\",\n",
        "    \"word2vec is a popular word\",\n",
        "]\n",
        "\n",
        "# Tokenize the text data (split sentences into words)\n",
        "tokenized_data = [sentence.split() for sentence in data]\n",
        "\n",
        "# Create a Word2Vec model\n",
        "w2v_model = Word2Vec(tokenized_data, min_count=0, workers=cpu_count())\n",
        "\n",
        "# Find the most similar words to 'word'\n",
        "similar_words = w2v_model.wv.most_similar('word')\n",
        "\n",
        "for word, score in similar_words:\n",
        "    print(f\"{word}: {score}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROt-cdpo4uFR",
        "outputId": "5695561b-8e2c-4d61-c805-81cf0326f041"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: 0.21883945167064667\n",
            "Apart: 0.21617142856121063\n",
            "data: 0.0931011214852333\n",
            "NLTK: 0.09291722625494003\n",
            "individual: 0.07963486760854721\n",
            "from: 0.06285078823566437\n",
            "is: 0.05433003976941109\n",
            "library: 0.0270574688911438\n",
            "the: 0.016134677454829216\n",
            "popular: -0.01083916611969471\n"
          ]
        }
      ]
    }
  ]
}